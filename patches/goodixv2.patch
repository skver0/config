--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -1560,6 +1560,7 @@
 
 int kvm_emulate_cpuid(struct kvm_vcpu *vcpu)
 {
+	vcpu->run->exit_reason = 123;
 	u32 eax, ebx, ecx, edx;
 
 	if (cpuid_fault_enabled(vcpu) && !kvm_require_cpl(vcpu, 0))
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -346,6 +346,8 @@
 	sigset_t sigset;
 	unsigned int halt_poll_ns;
 	bool valid_wakeup;
+	u64 last_exit_start;
+ u64 total_exit_time;
 
 #ifdef CONFIG_HAS_IOMEM
 	int mmio_needed;
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -1283,6 +1283,7 @@
 	svm_set_intercept(svm, INTERCEPT_XSETBV);
 	svm_set_intercept(svm, INTERCEPT_RDPRU);
 	svm_set_intercept(svm, INTERCEPT_RSM);
+	svm_set_intercept(svm, INTERCEPT_RDTSC);
 
 	if (!kvm_mwait_in_guest(vcpu->kvm)) {
 		svm_set_intercept(svm, INTERCEPT_MONITOR);
@@ -3235,6 +3236,24 @@
 	return kvm_handle_invpcid(vcpu, type, gva);
 }
 
+static int handle_rdtsc_interception(struct kvm_vcpu *vcpu) 
+{
+	u64 differece;
+	u64 final_time;
+	u64 data;
+
+	differece = rdtsc() - vcpu->last_exit_start;
+	final_time = vcpu->total_exit_time + differece;
+
+	data = rdtsc() - final_time;
+
+	vcpu->arch.regs[VCPU_REGS_RAX] = data & -1u;
+	vcpu->arch.regs[VCPU_REGS_RDX] = (data >> 32) & -1u;
+
+ vcpu->run->exit_reason = 123;
+	return svm_skip_emulated_instruction(vcpu);
+}
+
 static int (*const svm_exit_handlers[])(struct kvm_vcpu *vcpu) = {
 	[SVM_EXIT_READ_CR0]			= cr_interception,
 	[SVM_EXIT_READ_CR3]			= cr_interception,
@@ -3307,6 +3326,7 @@
 	[SVM_EXIT_AVIC_INCOMPLETE_IPI]		= avic_incomplete_ipi_interception,
 	[SVM_EXIT_AVIC_UNACCELERATED_ACCESS]	= avic_unaccelerated_access_interception,
 	[SVM_EXIT_VMGEXIT]			= sev_handle_vmgexit,
+	[SVM_EXIT_RDTSC]                = handle_rdtsc_interception,
 };
 
 static void dump_vmcb(struct kvm_vcpu *vcpu)
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -1118,6 +1118,7 @@
 
 int kvm_emulate_xsetbv(struct kvm_vcpu *vcpu)
 {
+	vcpu->run->exit_reason = 123;
 	/* Note, #UD due to CR4.OSXSAVE=0 has priority over the intercept. */
 	if (static_call(kvm_x86_get_cpl)(vcpu) != 0 ||
 	    __kvm_set_xcr(vcpu, kvm_rcx_read(vcpu), kvm_read_edx_eax(vcpu))) {
@@ -2106,6 +2107,7 @@
 
 int kvm_emulate_invd(struct kvm_vcpu *vcpu)
 {
+	vcpu->run->exit_reason = 123;
 	/* Treat an INVD instruction as a NOP and just skip it. */
 	return kvm_emulate_as_nop(vcpu);
 }
@@ -4154,6 +4156,9 @@
 
 int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
+	u64 differece;
+	u64 final_time;
+
 	switch (msr_info->index) {
 	case MSR_IA32_PLATFORM_ID:
 	case MSR_IA32_EBL_CR_POWERON:
@@ -4222,17 +4227,11 @@
 		 * return L1's TSC value to ensure backwards-compatible
 		 * behavior for migration.
 		 */
-		u64 offset, ratio;
+		differece = rdtsc() - vcpu->last_exit_start;
+		final_time = vcpu->total_exit_time + differece;
+		msr_info->data = rdtsc() - final_time;
 
-		if (msr_info->host_initiated) {
-			offset = vcpu->arch.l1_tsc_offset;
-			ratio = vcpu->arch.l1_tsc_scaling_ratio;
-		} else {
-			offset = vcpu->arch.tsc_offset;
-			ratio = vcpu->arch.tsc_scaling_ratio;
-		}
-
-		msr_info->data = kvm_scale_tsc(rdtsc(), ratio) + offset;
+		vcpu->run->exit_reason = 123;
 		break;
 	}
 	case MSR_IA32_CR_PAT:
@@ -8095,6 +8094,7 @@
 
 int kvm_emulate_wbinvd(struct kvm_vcpu *vcpu)
 {
+	vcpu->run->exit_reason = 123;
 	kvm_emulate_wbinvd_noskip(vcpu);
 	return kvm_skip_emulated_instruction(vcpu);
 }
@@ -10617,7 +10617,7 @@
  * exiting to the userspace.  Otherwise, the value will be returned to the
  * userspace.
  */
-static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
+static int vcpu_enter_guest_real(struct kvm_vcpu *vcpu)
 {
 	int r;
 	bool req_int_win =
@@ -11002,6 +11002,24 @@
 	return r;
 }
 
+static int vcpu_enter_guest(struct kvm_vcpu *vcpu) 
+{
+	int result;
+	u64 differece;
+
+	vcpu->last_exit_start = rdtsc();
+
+	result = vcpu_enter_guest_real(vcpu);
+
+	if (vcpu->run->exit_reason == 123) 
+	{
+		differece = rdtsc() - vcpu->last_exit_start;
+		vcpu->total_exit_time += differece;
+	}
+
+	return result;
+}
+
 /* Called within kvm->srcu read side.  */
 static inline int vcpu_block(struct kvm_vcpu *vcpu)
 {